{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ab1992d-b175-4c1f-aca0-badb3522244a",
      "metadata": {
        "id": "3ab1992d-b175-4c1f-aca0-badb3522244a"
      },
      "source": [
        "# Adaptation Prompt Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77e13e5-a892-4fcc-a25b-bf6c87d686aa",
      "metadata": {
        "id": "c77e13e5-a892-4fcc-a25b-bf6c87d686aa"
      },
      "source": [
        "In this notebook, we will look into how to perform adaptation prompt tuning for generating short Text Ads."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59b3388-e0f4-4627-b5cb-52c32556f070",
      "metadata": {
        "id": "c59b3388-e0f4-4627-b5cb-52c32556f070"
      },
      "source": [
        "Load the required libraries and the config parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fbfca2",
      "metadata": {
        "id": "71fbfca2",
        "outputId": "a24f474c-8694-44fa-b31c-eb67d9f92d8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/transformers/src/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "2024-01-01 20:50:10.030221: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-01 20:50:10.030270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-01 20:50:10.031141: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-01 20:50:10.037212: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-01 20:50:10.860288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-01-01 20:50:12,513] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"]=\"prompt_learning_methods\"\n",
        "from enum import Enum\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed\n",
        "from peft import get_peft_config, get_peft_model, AdaptionPromptConfig, TaskType, PeftType\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "device = \"cuda\"\n",
        "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer_name_or_path = \"meta-llama/Llama-2-7b-hf\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5bd732-dc51-40d7-8192-b878f2d55acd",
      "metadata": {
        "id": "2b5bd732-dc51-40d7-8192-b878f2d55acd"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa5ab45b-d0bf-428e-8732-0c06d0a7c2af",
      "metadata": {
        "id": "fa5ab45b-d0bf-428e-8732-0c06d0a7c2af"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a3648b",
      "metadata": {
        "id": "e1a3648b",
        "outputId": "79554ffb-2c32-46b5-c7cc-c54e3d6283ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'content': '<|im_start|>system\\nCreate a text ad given the following product and description.<|im_end|>\\n<|im_start|>user\\nProduct:  Harem pants\\nDescription:  A style of pants with a dropped crotch, loose-fitting legs, and a gathered waistband for a unique, bohemian look.\\n<|im_end|>\\n<|im_start|>assistant\\nAd: Discover Harem Pants! Unique, stylish bohemian vibes with a dropped crotch & loose legs. Comfy meets chic - elevate your wardrobe. Limited stock - shop now!\\n<|im_end|>\\n'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset_name = \"jaykin01/advertisement-copy\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\"\n",
        "tokenizer.chat_template = template\n",
        "\n",
        "system_prompt = \"\"\"Create a text ad given the following product and description.\"\"\"\n",
        "\n",
        "def preprocess(samples):\n",
        "    batch = []\n",
        "    for product, desc, ad_copy in zip(samples[\"product\"],samples[\"description\"],samples[\"ad\"]):\n",
        "        conversation = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Product: {product}\\nDescription: {desc}\\n\"\"\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"\"\"Ad: {ad_copy}\\n\"\"\"},\n",
        "        ]\n",
        "        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))\n",
        "    return {\"content\": batch}\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcde138-d315-4020-93df-b5a5186bd399",
      "metadata": {
        "id": "3dcde138-d315-4020-93df-b5a5186bd399",
        "outputId": "8ef3d201-17a4-4b86-8127-f46a87898688"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['content'],\n",
              "        num_rows: 1026\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['content'],\n",
              "        num_rows: 115\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset[\"train\"].train_test_split(0.1)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c99d71-1bb9-457a-9c88-71b886ddb8cd",
      "metadata": {
        "id": "75c99d71-1bb9-457a-9c88-71b886ddb8cd"
      },
      "source": [
        "## Create the PEFT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44e2c27-49fc-4abd-99b5-3d768d1489cb",
      "metadata": {
        "id": "e44e2c27-49fc-4abd-99b5-3d768d1489cb"
      },
      "source": [
        "### Adaptation Prompt Tuning config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b88f3a59-04e8-4d9f-9541-10b2d5785b66",
      "metadata": {
        "id": "b88f3a59-04e8-4d9f-9541-10b2d5785b66"
      },
      "outputs": [],
      "source": [
        "peft_config = AdaptionPromptConfig(adapter_len=32,\n",
        "                                   adapter_layers=30,\n",
        "                                   task_type=TaskType.CAUSAL_LM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9849ec3-f97b-48f9-876e-61022966bdde",
      "metadata": {
        "id": "f9849ec3-f97b-48f9-876e-61022966bdde"
      },
      "outputs": [],
      "source": [
        "class ChatmlSpecialTokens(str, Enum):\n",
        "    user = \"<|im_start|>user\"\n",
        "    assistant = \"<|im_start|>assistant\"\n",
        "    system = \"<|im_start|>system\"\n",
        "    eos_token = \"<|im_end|>\"\n",
        "    bos_token = \"<s>\"\n",
        "    pad_token = \"<pad>\"\n",
        "\n",
        "    @classmethod\n",
        "    def list(cls):\n",
        "        return [c.value for c in cls]\n",
        "response_template = \"<|im_start|>assistant\\n\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        pad_token=ChatmlSpecialTokens.pad_token.value,\n",
        "        bos_token=ChatmlSpecialTokens.bos_token.value,\n",
        "        eos_token=ChatmlSpecialTokens.eos_token.value,\n",
        "        additional_special_tokens=ChatmlSpecialTokens.list(),\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "tokenizer.chat_template = template\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a773e092",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7bde13aa80f64569b0e9668b5f0f1ec0"
          ]
        },
        "id": "a773e092",
        "outputId": "326ec1f8-02c6-4cf7-cb59-6b26adcc64fd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bde13aa80f64569b0e9668b5f0f1ec0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,932,190 || all params: 6,742,388,766 || trainable%: 0.05832042821127351\n"
          ]
        }
      ],
      "source": [
        "# creating model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# cast non-trainable params in fp16\n",
        "for p in model.parameters():\n",
        "    if not p.requires_grad:\n",
        "        p.data = p.to(torch.float16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151aa2b3",
      "metadata": {
        "id": "151aa2b3"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffa90e2-21a9-4a49-a9dd-c60049c08f3a",
      "metadata": {
        "id": "6ffa90e2-21a9-4a49-a9dd-c60049c08f3a"
      },
      "outputs": [],
      "source": [
        "output_dir = \"llama_adcopy\"\n",
        "per_device_train_batch_size = 8\n",
        "per_device_eval_batch_size = 8\n",
        "gradient_accumulation_steps = 1\n",
        "logging_steps = 5\n",
        "learning_rate = 5e-4\n",
        "max_grad_norm = 1.0\n",
        "max_steps = 250\n",
        "num_train_epochs=10\n",
        "warmup_ratio = 0.1\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_seq_length = 512\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    save_strategy=\"no\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    fp16=True,\n",
        "    report_to=[\"tensorboard\", \"wandb\"],\n",
        "    hub_private_repo=True,\n",
        "    push_to_hub=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f91568",
      "metadata": {
        "id": "b2f91568",
        "outputId": "561a4db8-00e3-4b2f-b40a-0b5ac0acede8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    packing=False,\n",
        "    dataset_text_field=\"content\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2733196b-ec43-4f64-ab77-8d7dc4593d0d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3255f10ae1e84a6494cde4df0bdf874d",
            "0dea4cc3c7de4fd0bd82ec573a3add5a",
            "1e96fb0302b3472089079a95e3d2df99",
            "35c92e8d900141728e1061cca792c45f"
          ]
        },
        "id": "2733196b-ec43-4f64-ab77-8d7dc4593d0d",
        "outputId": "ab1a7e3e-c371-4839-b6a9-e5518b577beb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msmangrul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.1 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/raid/sourab/temp/wandb/run-20240101_205027-2l36o5n4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/smangrul/prompt_learning_methods/runs/2l36o5n4' target=\"_blank\">good-lion-55</a></strong> to <a href='https://wandb.ai/smangrul/prompt_learning_methods' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/smangrul/prompt_learning_methods' target=\"_blank\">https://wandb.ai/smangrul/prompt_learning_methods</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/smangrul/prompt_learning_methods/runs/2l36o5n4' target=\"_blank\">https://wandb.ai/smangrul/prompt_learning_methods/runs/2l36o5n4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1290' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1290/1290 09:37, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.459700</td>\n",
              "      <td>1.455904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.895700</td>\n",
              "      <td>0.912726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.692900</td>\n",
              "      <td>0.819112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.665800</td>\n",
              "      <td>0.772601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.644300</td>\n",
              "      <td>0.752461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.685900</td>\n",
              "      <td>0.737771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.616100</td>\n",
              "      <td>0.729294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.625100</td>\n",
              "      <td>0.727424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.645400</td>\n",
              "      <td>0.725144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.578600</td>\n",
              "      <td>0.724965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/raid/sourab/peft/src/peft/utils/save_and_load.py:141: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3255f10ae1e84a6494cde4df0bdf874d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dea4cc3c7de4fd0bd82ec573a3add5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e96fb0302b3472089079a95e3d2df99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/4.73k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35c92e8d900141728e1061cca792c45f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "events.out.tfevents.1704138626.hf-dgx-01.2738331.0:   0%|          | 0.00/47.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c920f9d1",
      "metadata": {
        "id": "c920f9d1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e12d1e",
      "metadata": {
        "id": "61e12d1e"
      },
      "source": [
        "## Loading the trained model and getting the predictions of the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2d32a7-df76-4f57-8fa5-5a49aa8eb012",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a77da6b68f644ed1a65317df4f750819",
            "722b7142e19742c6a68d5052f35f9f8c",
            "bb08caea70084b9597774f4a3efe5c75"
          ]
        },
        "id": "3b2d32a7-df76-4f57-8fa5-5a49aa8eb012",
        "outputId": "dd0e9fef-2fd5-4f14-b3f4-a6725c0cc560"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a77da6b68f644ed1a65317df4f750819",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/269 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "722b7142e19742c6a68d5052f35f9f8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb08caea70084b9597774f4a3efe5c75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "peft_model_id = \"Sanjaytfg/llama_adcopy\"\n",
        "device = \"cuda\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24041ee1",
      "metadata": {
        "id": "24041ee1",
        "outputId": "a0bf2940-6866-42fb-db64-1e45a2e72dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s><|im_start|>system \n",
            "Create a text ad given the following product and description.<|im_end|> \n",
            "<|im_start|>user \n",
            "Product: Sony PS5 PlayStation Console\n",
            "Description: The PS5™ console unleashes new gaming possibilities that you never anticipated.<|im_end|> \n",
            "<|im_start|>assistant \n",
            "Ad: Unlock your gaming potential with the PS5! 🎮🌟 Experience next-gen gaming and endless entertainment. Perfect for gamers and immersing yourself in epic worlds. Limited stock - game on! 🌟🕹️🏆\n",
            "<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "model.to(torch.float16)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Create a text ad given the following product and description.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Product: Sony PS5 PlayStation Console\\nDescription: The PS5™ console unleashes new gaming possibilities that you never anticipated.\"},\n",
        "]\n",
        "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")#, add_special_tokens=False)\n",
        "inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n",
        "outputs = model.generate(**inputs,\n",
        "                         max_new_tokens=128,\n",
        "                         do_sample=True,\n",
        "                         top_p=0.95,\n",
        "                         temperature=0.2,\n",
        "                         repetition_penalty=1.1,\n",
        "                         eos_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bfcadc-1f06-48d9-ba71-b5f457b0f2f2",
      "metadata": {
        "id": "23bfcadc-1f06-48d9-ba71-b5f457b0f2f2"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}